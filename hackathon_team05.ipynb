{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1AosAX9DXOlc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yg0585/2022_cau_oss_hackathon/blob/main/hackathon_team05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        "*   모든 구현은 [2. 데이터 전처리] 및 [3.모델 생성]에서만 진행\n",
        "*   [4. 모델 저장]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *    트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임 -> 런타임 연결 해제 및 삭제\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        " *    \"런타임 -> 런타임 다시시작\"은 클라우드 스토리지에 저장된 모델은 유지됨\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        " *  모델 발표 자료 \n",
        "* 제출 기한: **오후 6시 (단, 발표자료는 12시)**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2022_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드가 적발될 경우\n",
        "*  Pre-trained 모델을 사용한 경우 (transfer learning 포함)\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2584fed3-c484-4b9e-ab2d-c929529ffb47"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "# 데이터셋 로드 (MNIST, fashion-MNIST, Kujushiji-MNIST, MNIST_corrupted (test only))\n",
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "\n",
        "train_ds2, test_ds2 = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds2['label'] += 10;\n",
        "test_ds2['label'] += 10;\n",
        "\n",
        "train_ds3, test_ds3 = tfds.load('kmnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds3['label'] += 20;\n",
        "test_ds3['label'] += 20;\n",
        "\n",
        "test_ds4 = tfds.load('mnist_corrupted/zigzag', split='test', shuffle_files=False, batch_size=-1)\n",
        "\n",
        "# 데이터셋 병합 (training: 180,000개, test: 40,000개)\n",
        "x_train = np.append(np.append(train_ds['image'], train_ds2['image'], 0), train_ds3['image'], 0);\n",
        "y_train = np.append(np.append(train_ds['label'], train_ds2['label'], 0), train_ds3['label'], 0);\n",
        "\n",
        "x_test = np.append(np.append(np.append(test_ds['image'], test_ds2['image'], 0), test_ds3['image'], 0), test_ds4['image'], 0);\n",
        "y_test = np.append(np.append(np.append(test_ds['label'], test_ds2['label'], 0), test_ds3['label'], 0), test_ds4['label'], 0);\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수: 30, 입력 데이터 구조: (28, 28, 1)\n",
        "num_classes = y_train.shape[1]\n",
        "input_shape = x_train.shape[1:]\n",
        "print(num_classes, input_shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 (28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9"
      },
      "source": [
        "# **2. 데이터 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9KWTBP6AI1"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "# 원본 데이터와 전처리 후 데이터를 구분하기 위해, 변수명 x_train_after, x_test_after를 변경하지 말 것\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "\n",
        "#horizontal_flip=True, vertical_flip=True,height_shift_range=0.1, width_shift_range=0.1, shear_range=0.1, zoom_range=[0.7, 1.3]\n",
        "                                                       \n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(height_shift_range=0.1, width_shift_range=0.1, shear_range=0.1, zoom_range=[0.7, 1.3], rotation_range=15\n",
        "                                                       )\n",
        "\n",
        "datagen.fit(x_train_after)\n",
        "# x_train_after = x_train / 255.0\n",
        "# x_test_after = x_test / 255.0\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-lo-O1yiFpY"
      },
      "source": [
        "# **3. 모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a6106c-ebf7-4728-d1f7-96929afa5632"
      },
      "source": [
        "from tensorflow.python.keras.layers.advanced_activations import regularizers\n",
        "# 순차 모델 생성 (가장 기본구조)\n",
        "\n",
        "\n",
        "lalpha=0.2\n",
        "l2w = 0.0006\n",
        "lr = 0.0001\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, input_shape=(28,28,1), padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "#model.add(keras.layers.LeakyReLU(alpha=lalpha))\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)))\n",
        "\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))   \n",
        "\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,  padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)\n",
        "                              ))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,  padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)\n",
        "                              ))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=1,  padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)\n",
        "                              ))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=1,  padding='same', kernel_regularizer=keras.regularizers.l2(0.0005)\n",
        "                              ))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "#model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# model.add(keras.layers.Conv2D(filters=256, kernel_size=3, strides=1,  padding='same', \n",
        "#                               ))\n",
        "# model.add(keras.layers.BatchNormalization())\n",
        "# model.add(keras.layers.Activation('relu'))\n",
        "\n",
        "# model.add(keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, input_shape=(28,28,1), padding='same',\n",
        "#                               ))\n",
        "\n",
        "# model.add(keras.layers.BatchNormalization())\n",
        "# model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten layer: 28 x 28 x 1 image를 1D vector input으로 변환\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# 1st hidden layer: fully-connected layer\n",
        "# model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "# model.add(keras.layers.BatchNormalization())\n",
        "# model.add(keras.layers.Activation('relu'))\n",
        "\n",
        "# # 3rd hidden layer: fully-connected layer \n",
        "model.add(keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "# Output layer: fully-connected layer \n",
        "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "# # 모델 컴파일\n",
        "# # optimizer: 모델을 업데이트 하는 방식\n",
        "# # loss: 모델의 정확도를 판단하는 방식\n",
        "# # metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=lr, momentum=0.9, decay=0.0004), metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='max')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set (x_test1_after, x_test2_after, 혹은 둘을 merge해서 사용)\n",
        "def scheduler(epoch):\n",
        "  if epoch > 7 :\n",
        "    return lr / 10\n",
        "  return lr\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "model.fit(x_train_after, y_train, batch_size = 64, epochs = 20, shuffle=True, callbacks=[cp_callback, callback], validation_data=(x_test_after, y_test))\n",
        "#model.fit(datagen.flow(x_train_after, y_train, batch_size = 128, shuffle=True), epochs = 100, shuffle=True, callbacks=[cp_callback, callback], validation_data=(x_test_after, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 28, 28, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               802944    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 30)                3870      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,111,550\n",
            "Trainable params: 1,110,654\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.9426 - accuracy: 0.7671\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85740, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 41s 13ms/step - loss: 0.9426 - accuracy: 0.7671 - val_loss: 0.6336 - val_accuracy: 0.8574 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.4370 - accuracy: 0.9080\n",
            "Epoch 2: val_accuracy improved from 0.85740 to 0.87612, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.4370 - accuracy: 0.9080 - val_loss: 0.5840 - val_accuracy: 0.8761 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.9304\n",
            "Epoch 3: val_accuracy improved from 0.87612 to 0.89022, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.3462 - accuracy: 0.9303 - val_loss: 0.5142 - val_accuracy: 0.8902 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.9405\n",
            "Epoch 4: val_accuracy improved from 0.89022 to 0.90478, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.2970 - accuracy: 0.9405 - val_loss: 0.4497 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9468\n",
            "Epoch 5: val_accuracy did not improve from 0.90478\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.2611 - accuracy: 0.9468 - val_loss: 0.4264 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9516\n",
            "Epoch 6: val_accuracy did not improve from 0.90478\n",
            "2813/2813 [==============================] - 39s 14ms/step - loss: 0.2332 - accuracy: 0.9516 - val_loss: 0.5181 - val_accuracy: 0.8925 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9552\n",
            "Epoch 7: val_accuracy did not improve from 0.90478\n",
            "2813/2813 [==============================] - 36s 13ms/step - loss: 0.2119 - accuracy: 0.9553 - val_loss: 0.5871 - val_accuracy: 0.8805 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9577\n",
            "Epoch 8: val_accuracy improved from 0.90478 to 0.90753, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1967 - accuracy: 0.9577 - val_loss: 0.4266 - val_accuracy: 0.9075 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9658\n",
            "Epoch 9: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1669 - accuracy: 0.9658 - val_loss: 0.4982 - val_accuracy: 0.8978 - lr: 1.0000e-05\n",
            "Epoch 10/20\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9675\n",
            "Epoch 10: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1597 - accuracy: 0.9675 - val_loss: 0.4783 - val_accuracy: 0.9010 - lr: 1.0000e-05\n",
            "Epoch 11/20\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1575 - accuracy: 0.9685\n",
            "Epoch 11: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1575 - accuracy: 0.9685 - val_loss: 0.4853 - val_accuracy: 0.8985 - lr: 1.0000e-05\n",
            "Epoch 12/20\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9690\n",
            "Epoch 12: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1546 - accuracy: 0.9690 - val_loss: 0.5133 - val_accuracy: 0.8942 - lr: 1.0000e-05\n",
            "Epoch 13/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9694\n",
            "Epoch 13: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1525 - accuracy: 0.9694 - val_loss: 0.4920 - val_accuracy: 0.8968 - lr: 1.0000e-05\n",
            "Epoch 14/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9698\n",
            "Epoch 14: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1500 - accuracy: 0.9698 - val_loss: 0.5126 - val_accuracy: 0.8956 - lr: 1.0000e-05\n",
            "Epoch 15/20\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9704\n",
            "Epoch 15: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1474 - accuracy: 0.9704 - val_loss: 0.5023 - val_accuracy: 0.8963 - lr: 1.0000e-05\n",
            "Epoch 16/20\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9702\n",
            "Epoch 16: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1457 - accuracy: 0.9702 - val_loss: 0.5175 - val_accuracy: 0.8929 - lr: 1.0000e-05\n",
            "Epoch 17/20\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9709\n",
            "Epoch 17: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1439 - accuracy: 0.9709 - val_loss: 0.4894 - val_accuracy: 0.8982 - lr: 1.0000e-05\n",
            "Epoch 18/20\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9714\n",
            "Epoch 18: val_accuracy did not improve from 0.90753\n",
            "2813/2813 [==============================] - 37s 13ms/step - loss: 0.1417 - accuracy: 0.9714 - val_loss: 0.5173 - val_accuracy: 0.8944 - lr: 1.0000e-05\n",
            "Epoch 19/20\n",
            "1924/2813 [===================>..........] - ETA: 10s - loss: 0.1399 - accuracy: 0.9720"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CKztKsFQjMfR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team05'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPbgI-c-Kj8"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WONVxH-Kt6"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team05'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "model.summary()\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}